{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turing-usp/RL_Truco/blob/main/Truco_Deep_Sarsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ambiente"
      ],
      "metadata": {
        "id": "i2GHEitmTNHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "tYkDMhczTVXl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário do deck (apenas usado pra visualização se quiser e não internamente)\n",
        "dict_deck = {\n",
        "    14: '4p',  # 4 de paus (Zap)\n",
        "    13: '7c',  # 7 de copas (Copeta)\n",
        "    12: 'Ae',  # Ás de espadas (Espadilha)\n",
        "    11: '7o',  # 7 de ouros (Ourito)\n",
        "    10: '3',\n",
        "    9: '2',\n",
        "    8: 'A',\n",
        "    7: 'K',\n",
        "    6: 'J',\n",
        "    5: 'Q',\n",
        "    4: '7',\n",
        "    3: '6',\n",
        "    2: '5',\n",
        "    1: '4',\n",
        "    0: 'Carta já jogada'\n",
        "}"
      ],
      "metadata": {
        "id": "quWNbYKcTche"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrucoMineiroEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    # Cria o deck\n",
        "    self.deck =  self._create_deck()\n",
        "    # Contador de mãos jogadas\n",
        "    self.turn = 0\n",
        "    # Placar 0=oponente, 1=agente\n",
        "    self.score = [0,0]\n",
        "    # Aleatoriza quem começa (0=oponente, 1=agente)\n",
        "    self.first_player = random.randint(0, 1)\n",
        "    # Definindo o espaço de ação (0, 1, 2 representam as cartas na mão do agente)\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "    # Definindo o espaço de observação (carta jogada pelo oponente, cartas na mão do agente, estado da primeira mão)\n",
        "    self.observation_space = spaces.Tuple((\n",
        "      spaces.Discrete(15),  # Carta jogada pelo oponente, 0 se for a vez do agente\n",
        "      spaces.MultiDiscrete([15]*3),  # Cartas na mão do agente (0 representa carta jogada)\n",
        "      spaces.Discrete(4)  # Estado da primeira mão (0 - essa é a primeira mão, 1 - oponente ganhou, 2 - empate, 3 - agente ganhou)\n",
        "    ))\n",
        "    self.observation_space.n = 15 * 15 * 15 * 15 * 4\n",
        "    # Variáveis de estado\n",
        "    self.opponent_card = 0 if self.first_player == 1 else self.draw() # Carta jogada pelo oponente (nenhuma ou aleatório)\n",
        "    self.agent_cards = np.sort([self.draw(), self.draw(), self.draw()])[::-1]\n",
        "    # Agente compra 3 cartas\n",
        "    self.first_hand_winner = 0  # Estado da primeira mão\n",
        "\n",
        "  def _create_deck(self):\n",
        "    # Deck de cartas com 4p=14 > 7c=13 > Ae=12 > 7o=11 > 3=10 > 2=9 > A=8 > K=7 > J=6 > Q=5 > 7=4 > 6=3 > 5=2 > 4=1\n",
        "    # 1 de cada manilha, 3 cartas A, 3 cartas 4, 2 cartas 7 e as 4 das demais\n",
        "    return 1*[14] + 1*[13] + 1*[12] + 1*[11] + 4*[10] + 4*[9] + 3*[8] + 4*[7] + 4*[6] + 4*[5] + 2*[4] + 4*[3] + 4*[2] + 3*[1]\n",
        "\n",
        "  def draw(self):\n",
        "    # Compra uma carta do deck\n",
        "    if self.deck:\n",
        "      card_index = random.randint(0, len(self.deck) - 1)\n",
        "      card = self.deck.pop(card_index)\n",
        "      return card\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def step(self, action):\n",
        "    # Verifica se a ação é válida (0, 1 ou 2)\n",
        "    if action not in [0, 1, 2]:\n",
        "      raise ValueError(\"Invalid action. Action must be 0, 1, or 2.\")\n",
        "\n",
        "    # Executa a ação (joga uma carta)\n",
        "    player_card = self.agent_cards[action]\n",
        "    self.agent_cards[action] = 0  # Marca a carta como jogada\n",
        "\n",
        "    # Se não tiver ação do oponente, joga uma carta aleatória para ele\n",
        "    if self.opponent_card == 0:\n",
        "      self.opponent_card = self.draw()\n",
        "\n",
        "    # Se alguém não jogou, levanta erros (não é para acontecer)\n",
        "    if self.opponent_card == 0:\n",
        "      raise ValueError(\"Opponent has no card set.\")\n",
        "    if player_card == 0:\n",
        "      raise ValueError(\"Player has no card set.\")\n",
        "\n",
        "    # Determina o vencedor da mão\n",
        "    hand_winner = self._determine_hand_winner(self.opponent_card, player_card)\n",
        "\n",
        "    # Determina o vencedor da rodada, se existir e atualiza o placar\n",
        "    round_winner = self._determine_round_winner(hand_winner)\n",
        "    if round_winner == 1 or round_winner == 3:\n",
        "      self.score[0 if round_winner == 1 else 1] += 1\n",
        "\n",
        "    # Determina quem ganhou a primeira mão se estiver nela\n",
        "    if self.turn == 0:\n",
        "      self.first_hand_winner = hand_winner\n",
        "\n",
        "    # Define quem começa jogando a próxima mão (mantém em caso de empate, senão quem ganhou começa a próxima)\n",
        "    if hand_winner != 2:\n",
        "      self.first_player = min(hand_winner - 1, 1)\n",
        "\n",
        "    # Se o oponente começar, ele joga uma carta aleatória, senão ele começa sem carta\n",
        "    if self.first_player == 0:\n",
        "      self.opponent_card = self.draw()\n",
        "    else:\n",
        "      self.opponent_card = 0\n",
        "\n",
        "    # Avança o turno\n",
        "    self.turn += 1\n",
        "\n",
        "    # Determina a recompensa (0 para empates ou rodada inacabada, +1 vitória, -1 derrota)\n",
        "    reward = 0 if round_winner == 0 else round_winner - 2\n",
        "\n",
        "    # Sort na mão do agente\n",
        "    self.agent_cards = np.sort(self.agent_cards)[::-1]\n",
        "\n",
        "    # Retorna a observação, a recompensa (-1, 0 ou 1) se a rodada acabou ou 0 se a rodada não acabou e a flag de rodada acabada\n",
        "    observation = (self.opponent_card, self.agent_cards, self.first_hand_winner)\n",
        "    if round_winner != 0:\n",
        "      self.reset(reset_score=False)\n",
        "    done = False if 12 not in self.score else True\n",
        "    return {'observation' : observation, 'reward' : reward, 'done' : done}\n",
        "\n",
        "  def reset(self, seed = None, reset_score = True):\n",
        "    # Reseta o ambiente\n",
        "    super().reset(seed=seed)\n",
        "    self.deck = self._create_deck()\n",
        "    self.turn = 0\n",
        "    if reset_score:\n",
        "      self.score = [0,0]\n",
        "    self.first_player = random.randint(0, 1)\n",
        "    self.opponent_card = 0 if self.first_player == 1 else self.draw()\n",
        "    self.agent_cards = np.sort([self.draw(), self.draw(), self.draw()])[::-1]\n",
        "    self.first_hand_winner = 0\n",
        "    return {'oberservation' : (self.opponent_card, self.agent_cards, self.first_hand_winner)}\n",
        "\n",
        "  def _determine_hand_winner(self, opponent_card, agent_card):\n",
        "    # Lógica para determinar o vencedor de uma mão (1=oponente ganha 2=empate 3=agente ganha)\n",
        "    if agent_card > opponent_card:\n",
        "      return 3\n",
        "    if agent_card < opponent_card:\n",
        "      return 1\n",
        "    return 2\n",
        "\n",
        "  def _determine_round_winner(self, hand_winner):\n",
        "    # Lógica para determinar o vencedor de uma rodada (0=indeterminado 1=oponente ganha 2=empate 3=agente ganha)\n",
        "    if self.turn == 2 or self.first_hand_winner == 2: # Terceiro turno ou primeira mão empatou\n",
        "      return hand_winner\n",
        "    if self.first_hand_winner == 1 and hand_winner != 3: # Oponente ganha primeira mão\n",
        "      return 1\n",
        "    if self.first_hand_winner == 3 and hand_winner != 1: # Agente ganha primeira mão\n",
        "      return 3\n",
        "    return 0 # Default"
      ],
      "metadata": {
        "id": "tIkETf3_TfS2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testes no ambiente\n",
        "truco = TrucoMineiroEnv()\n",
        "observation = truco.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    print(f\"Observação (opponent_card, agent_cards[], first_hand_winner): {observation}\")\n",
        "    print(f\"Cartas do agente: {[dict_deck[card_value] for card_value in truco.agent_cards]}\")\n",
        "    if dict_deck[truco.opponent_card] == 'Carta indisponível':\n",
        "        print(f\"Carta do oponente: ele joga depois\")\n",
        "    else:\n",
        "        print(f\"Carta do oponente: {dict_deck[truco.opponent_card]}\")\n",
        "    while True:\n",
        "        action = random.randint(0, 2)  # Escolhe uma ação aleatória (trocar pelo agente)\n",
        "        if truco.agent_cards[action] != 0:\n",
        "            break\n",
        "    print(f\"Carta jogada pelo agente: {dict_deck[truco.agent_cards[action]]}\")\n",
        "    result = truco.step(action)\n",
        "    observation, reward, done = result['observation'], result['reward'], result['done']\n",
        "    total_reward += reward\n",
        "\n",
        "    print(f\"Recompensa obtida neste passo: {reward}\")\n",
        "    print(f\"Recompensa acumulada: {total_reward}\")\n",
        "    print(f\"Placar: Agente {truco.score[1]} x {truco.score[0]} Oponente\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSKHcAUsTjp_",
        "outputId": "f5981f8b-4c08-404b-aefb-822e328fb2e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observação (opponent_card, agent_cards[], first_hand_winner): {'oberservation': (0, array([13,  5,  2]), 0)}\n",
            "Cartas do agente: ['7c', 'Q', '5']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 0 x 0 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (9, array([13,  2,  0]), 1)\n",
            "Cartas do agente: ['7c', '5', 'Carta já jogada']\n",
            "Carta do oponente: 2\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 0 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (8, array([13,  0,  0]), 1)\n",
            "Cartas do agente: ['K', 'Q', 'Q']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: K\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 0 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([5, 5, 0]), 3)\n",
            "Cartas do agente: ['Q', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 0 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (2, array([5, 0, 0]), 3)\n",
            "Cartas do agente: ['Q', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: 5\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 1 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([0, 0, 0]), 3)\n",
            "Cartas do agente: ['7c', 'A', '6']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: 6\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 1 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (7, array([13,  8,  0]), 1)\n",
            "Cartas do agente: ['7c', 'A', 'Carta já jogada']\n",
            "Carta do oponente: K\n",
            "Carta jogada pelo agente: 7c\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 1 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([8, 0, 0]), 1)\n",
            "Cartas do agente: ['A', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: A\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 2 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([0, 0, 0]), 1)\n",
            "Cartas do agente: ['Ae', '3', '5']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: Ae\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 2 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([10,  2,  0]), 3)\n",
            "Cartas do agente: ['3', '5', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 3\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 2\n",
            "Placar: Agente 3 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([2, 0, 0]), 3)\n",
            "Cartas do agente: ['2', 'Q', '4']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 2\n",
            "Placar: Agente 3 x 1 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (10, array([9, 1, 0]), 1)\n",
            "Cartas do agente: ['2', '4', 'Carta já jogada']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: 4\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 3 x 2 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (2, array([9, 0, 0]), 1)\n",
            "Cartas do agente: ['J', '6', '4']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 6\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 3 x 2 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (6, array([6, 1, 0]), 1)\n",
            "Cartas do agente: ['J', '4', 'Carta já jogada']\n",
            "Carta do oponente: J\n",
            "Carta jogada pelo agente: J\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 3 x 3 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (9, array([1, 0, 0]), 1)\n",
            "Cartas do agente: ['7c', 'J', '6']\n",
            "Carta do oponente: 4p\n",
            "Carta jogada pelo agente: 7c\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 3 x 3 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (10, array([6, 3, 0]), 1)\n",
            "Cartas do agente: ['J', '6', 'Carta já jogada']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: J\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 3 x 4 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (10, array([3, 0, 0]), 1)\n",
            "Cartas do agente: ['J', 'Q', '4']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 3 x 4 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (7, array([6, 1, 0]), 1)\n",
            "Cartas do agente: ['J', '4', 'Carta já jogada']\n",
            "Carta do oponente: K\n",
            "Carta jogada pelo agente: 4\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 3 x 5 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (5, array([6, 0, 0]), 1)\n",
            "Cartas do agente: ['3', '2', 'A']\n",
            "Carta do oponente: 4\n",
            "Carta jogada pelo agente: A\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 3 x 5 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([10,  9,  0]), 3)\n",
            "Cartas do agente: ['3', '2', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 3\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 4 x 5 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([9, 0, 0]), 3)\n",
            "Cartas do agente: ['K', 'Q', 'Q']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 4 x 5 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (5, array([7, 5, 0]), 1)\n",
            "Cartas do agente: ['K', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Q\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 4 x 6 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (7, array([7, 0, 0]), 1)\n",
            "Cartas do agente: ['7o', 'K', 'Q']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 4 x 6 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (2, array([11,  7,  0]), 1)\n",
            "Cartas do agente: ['7o', 'K', 'Carta já jogada']\n",
            "Carta do oponente: 5\n",
            "Carta jogada pelo agente: 7o\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 4 x 6 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([7, 0, 0]), 1)\n",
            "Cartas do agente: ['K', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: K\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 4 x 7 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (7, array([0, 0, 0]), 1)\n",
            "Cartas do agente: ['Q', '5', '5']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 4 x 7 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (2, array([5, 2, 0]), 1)\n",
            "Cartas do agente: ['Q', '5', 'Carta já jogada']\n",
            "Carta do oponente: 5\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 4 x 7 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([2, 0, 0]), 1)\n",
            "Cartas do agente: ['5', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -4\n",
            "Placar: Agente 4 x 8 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (8, array([0, 0, 0]), 1)\n",
            "Cartas do agente: ['3', 'K', 'Q']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 3\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -4\n",
            "Placar: Agente 4 x 8 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([7, 5, 0]), 2)\n",
            "Cartas do agente: ['K', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: Q\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -5\n",
            "Placar: Agente 4 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (5, array([7, 0, 0]), 2)\n",
            "Cartas do agente: ['3', '2', 'K']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 3\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -5\n",
            "Placar: Agente 4 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([9, 7, 0]), 3)\n",
            "Cartas do agente: ['2', 'K', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 2\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: -4\n",
            "Placar: Agente 5 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([7, 0, 0]), 3)\n",
            "Cartas do agente: ['A', 'K', 'J']\n",
            "Carta do oponente: 5\n",
            "Carta jogada pelo agente: J\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -4\n",
            "Placar: Agente 5 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([8, 7, 0]), 3)\n",
            "Cartas do agente: ['A', 'K', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: K\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 6 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([8, 0, 0]), 3)\n",
            "Cartas do agente: ['4p', '7c', '7']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 7\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 6 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (10, array([14, 13,  0]), 1)\n",
            "Cartas do agente: ['4p', '7c', 'Carta já jogada']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: 4p\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -3\n",
            "Placar: Agente 6 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([13,  0,  0]), 1)\n",
            "Cartas do agente: ['7c', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 7c\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 7 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([0, 0, 0]), 1)\n",
            "Cartas do agente: ['2', '2', 'J']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 2\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -2\n",
            "Placar: Agente 7 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([9, 6, 0]), 3)\n",
            "Cartas do agente: ['2', 'J', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: J\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 8 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([9, 0, 0]), 3)\n",
            "Cartas do agente: ['Ae', '7o', 'K']\n",
            "Carta do oponente: 5\n",
            "Carta jogada pelo agente: K\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 8 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([12, 11,  0]), 3)\n",
            "Cartas do agente: ['Ae', '7o', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 7o\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 9 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([12,  0,  0]), 3)\n",
            "Cartas do agente: ['7c', 'K', 'Q']\n",
            "Carta do oponente: 6\n",
            "Carta jogada pelo agente: K\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 9 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([13,  5,  0]), 3)\n",
            "Cartas do agente: ['7c', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 7c\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 10 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([5, 0, 0]), 3)\n",
            "Cartas do agente: ['7c', '7', '5']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 10 x 9 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (14, array([13,  4,  0]), 1)\n",
            "Cartas do agente: ['7c', '7', 'Carta já jogada']\n",
            "Carta do oponente: 4p\n",
            "Carta jogada pelo agente: 7c\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 10 x 10 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (7, array([4, 0, 0]), 1)\n",
            "Cartas do agente: ['2', '5', '5']\n",
            "Carta do oponente: A\n",
            "Carta jogada pelo agente: 2\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 10 x 10 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([2, 2, 0]), 3)\n",
            "Cartas do agente: ['5', '5', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 10 x 10 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (4, array([2, 0, 0]), 3)\n",
            "Cartas do agente: ['5', 'Carta já jogada', 'Carta já jogada']\n",
            "Carta do oponente: 7\n",
            "Carta jogada pelo agente: 5\n",
            "Recompensa obtida neste passo: -1\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 10 x 11 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (12, array([0, 0, 0]), 3)\n",
            "Cartas do agente: ['2', 'A', 'Q']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: A\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: -1\n",
            "Placar: Agente 10 x 11 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([9, 5, 0]), 3)\n",
            "Cartas do agente: ['2', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Carta já jogada\n",
            "Carta jogada pelo agente: 2\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 11 x 11 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (0, array([5, 0, 0]), 3)\n",
            "Cartas do agente: ['4p', '3', 'Q']\n",
            "Carta do oponente: 3\n",
            "Carta jogada pelo agente: 3\n",
            "Recompensa obtida neste passo: 0\n",
            "Recompensa acumulada: 0\n",
            "Placar: Agente 11 x 11 Oponente\n",
            "\n",
            "Observação (opponent_card, agent_cards[], first_hand_winner): (5, array([14,  5,  0]), 2)\n",
            "Cartas do agente: ['4p', 'Q', 'Carta já jogada']\n",
            "Carta do oponente: Q\n",
            "Carta jogada pelo agente: 4p\n",
            "Recompensa obtida neste passo: 1\n",
            "Recompensa acumulada: 1\n",
            "Placar: Agente 12 x 11 Oponente\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctg9zxuEI_3Z"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OUKiIm-JI_3Z"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn as nn\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wCg5jRUI_3a"
      },
      "source": [
        "## Create and prepare the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apbob5-dI_3a"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = TrucoMineiroEnv()"
      ],
      "metadata": {
        "id": "loIsVY2iWQzg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G-XpsiMYI_3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcd5cef-45ec-4381-8224-0f74505e02ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MountainCar env: State dimensions: 202500, Number of actions: 3\n"
          ]
        }
      ],
      "source": [
        "state_dims = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "print(f\"MountainCar env: State dimensions: {state_dims}, Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnfKvF-uI_3b"
      },
      "source": [
        "### Prepare the environment to work with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tc3FyCcI_3c"
      },
      "outputs": [],
      "source": [
        "class PreprocessEnv(gym.Wrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return torch.from_numpy(obs).unsqueeze(dim=0).float()\n",
        "\n",
        "    def step(self, action):\n",
        "        action = action.item()\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
        "        reward = torch.tensor(reward).view(1, -1).float()\n",
        "        done = torch.tensor(done).view(1, -1)\n",
        "        return next_state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K24l42NqI_3c"
      },
      "outputs": [],
      "source": [
        "env = PreprocessEnv(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdaT05K8I_3c"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "action = torch.tensor(0)\n",
        "next_state, reward, done, _ = env.step(action)\n",
        "print(f\"Sample state: {state}\")\n",
        "print(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXihRkbGI_3c"
      },
      "source": [
        "## Create the Q-Network and policy\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLKZe-pmI_3d"
      },
      "source": [
        "### Create the Q-Network: $\\hat q(s,a| \\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBVdTXmqI_3d"
      },
      "outputs": [],
      "source": [
        "q_network = nn.Sequential(\n",
        "    nn.Linear(state_dims, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, num_actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A06QbeZ4I_3d"
      },
      "source": [
        "### Create the target Q-Network: $\\hat q(s, a|\\theta_{targ})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giiJkUxmI_3d"
      },
      "outputs": [],
      "source": [
        "target_q_network = copy.deepcopy(q_network).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-bq_Gg8I_3d"
      },
      "source": [
        "### Create the $\\epsilon$-greedy policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFV2Gsa4I_3e"
      },
      "outputs": [],
      "source": [
        "def policy(state, epsilon=0.):\n",
        "    if torch.rand(1) < epsilon:\n",
        "        return torch.randint(num_actions, (1, 1))\n",
        "    else:\n",
        "        av = q_network(state).detach()\n",
        "        return torch.argmax(av, dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnCuL0reI_3e"
      },
      "source": [
        "### Plot the cost to go: $ - \\max_a \\hat q(s,a|\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWHW3CxPI_3e"
      },
      "outputs": [],
      "source": [
        "plot_cost_to_go(env, q_network, xlabel='Car Position', ylabel='Velocity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHXYiYt-I_3e"
      },
      "source": [
        "## Create the Experience Replay buffer\n",
        "\n",
        "<br>\n",
        "<div style=\"text-align:center\">\n",
        "    <p>A simple buffer that stores transitions of arbitrary values, adapted from\n",
        "    <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\">this source.</a></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-xAxpkLI_3e"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity=1000000):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def insert(self, transition):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = transition\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        assert self.can_sample(batch_size)\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        batch = zip(*batch)\n",
        "        return [torch.cat(items) for items in batch]\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size * 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-I6QfkqI_3e"
      },
      "source": [
        "## Implement the algorithm\n",
        "\n",
        "</br></br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nSDqmbaI_3e"
      },
      "outputs": [],
      "source": [
        "def deep_sarsa(q_network, policy, episodes, alpha=0.001,\n",
        "               batch_size=32, gamma=0.99, epsilon=0.05):\n",
        "    optim = AdamW(q_network.parameters(), lr=alpha)\n",
        "    memory = ReplayMemory()\n",
        "    stats = {'MSE Loss': [], 'Returns': []}\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0\n",
        "        while not done:\n",
        "            action = policy(state, epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            memory.insert([state, action, reward, done, next_state])\n",
        "\n",
        "            if memory.can_sample(batch_size):\n",
        "                state_b, action_b, reward_b, done_b, next_state_b = memory.sample(batch_size)\n",
        "                qsa_b = q_network(state_b).gather(1, action_b)\n",
        "                next_action_b = policy(next_state_b, epsilon)\n",
        "                next_qsa_b = target_q_network(next_state_b).gather(1, next_action_b)\n",
        "                target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
        "                loss = F.mse_loss(qsa_b, target_b)\n",
        "                q_network.zero_grad()\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "                stats['MSE Loss'].append(loss.item())\n",
        "\n",
        "            state = next_state\n",
        "            ep_return += reward.item()\n",
        "\n",
        "        stats['Returns'].append(ep_return)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "55v03cjFI_3e"
      },
      "outputs": [],
      "source": [
        "stats = deep_sarsa(q_network, policy, 2500, epsilon=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPe6EfdhI_3e"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhTAl32hI_3e"
      },
      "source": [
        "### Plot execution stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7wOzLBTI_3e"
      },
      "outputs": [],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV-guw7II_3e"
      },
      "source": [
        "### Plot the cost to go: $ - \\max_a \\hat q(s,a|\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKp-7t4SI_3f"
      },
      "outputs": [],
      "source": [
        "plot_cost_to_go(env, q_network, xlabel='Car Position', ylabel='Velocity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcC52BmbI_3f"
      },
      "source": [
        "### Show resulting policy: $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9sYjDFeI_3f"
      },
      "outputs": [],
      "source": [
        "plot_max_q(env, q_network, xlabel='Car Position', ylabel='Velocity',\n",
        "           action_labels=['Back', 'Do nothing', 'Forward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lnUdHHTI_3f"
      },
      "source": [
        "### Test the resulting agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhCD24CMI_3f"
      },
      "outputs": [],
      "source": [
        "test_agent(env, policy, episodes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFVeLHTSI_3f"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiLkSo28I_3g"
      },
      "source": [
        "[[1] Deep Reinforcement Learning with Experience Replay Based on SARSA](https://www.researchgate.net/publication/313803199_Deep_reinforcement_learning_with_experience_replay_based_on_SARSA)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}